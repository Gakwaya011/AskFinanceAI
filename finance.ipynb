{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2124705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a67ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c15cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('majorSeaweed/financeQA_100K')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3708044e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'context', 'answer'],\n",
       "        num_rows: 90566\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'context', 'answer'],\n",
       "        num_rows: 5031\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'context', 'answer'],\n",
       "        num_rows: 5032\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e750b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = load_dataset('majorSeaweed/financeQA_100K', split = 'train')\n",
    "data_val = load_dataset('majorSeaweed/financeQA_100K', split = 'validation')\n",
    "data_test = load_dataset('majorSeaweed/financeQA_100K', split = 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85038840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'context', 'answer'],\n",
      "    num_rows: 90566\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'context', 'answer'],\n",
      "    num_rows: 5031\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'context', 'answer'],\n",
      "    num_rows: 5032\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(data_tr)\n",
    "print(data_val)\n",
    "print(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ced40fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 17:11:02.512196: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-09 17:11:10.155470: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-09 17:11:24.717173: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b44a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "597a0824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5000\n",
      "Validation samples: 1000\n",
      "Test samples: 1000\n"
     ]
    }
   ],
   "source": [
    "# Take small subsets for fast trainingtrain_data = data['train'].select(range(5000))  # Good for substantial training\n",
    "train_data = data['train'].select(range(5000))  # Good for substantial training\n",
    "val_data = data['validation'].select(range(1000))  # Better validation\n",
    "test_data = data['test'].select(range(1000))  # Proper testing\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c58dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Initialize tokenizer FIRST\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36f7d847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXAMINING TOKENIZED DATA QUALITY ===\n",
      "Sample tokenized examples:\n",
      "\n",
      "--- Example 1 ---\n",
      "Original question: What is the total estimated project cost mentioned in the document?...\n",
      "Original answer: The grand total of the estimated project cost is $7,594,720....\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_train_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_data[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m100\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_data[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m100\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenized input_ids length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenized_train_list\u001b[49m[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenized attention_mask length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tokenized_train_list[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Decode back to text to see if it makes sense\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_train_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's examine what's actually in our tokenized data\n",
    "print(\"=== EXAMINING TOKENIZED DATA QUALITY ===\")\n",
    "\n",
    "# Check a few examples to see if they look right\n",
    "print(\"Sample tokenized examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Original question: {train_data[i]['question'][:100]}...\")\n",
    "    print(f\"Original answer: {train_data[i]['answer'][:100]}...\")\n",
    "    print(f\"Tokenized input_ids length: {len(tokenized_train_list[i]['input_ids'])}\")\n",
    "    print(f\"Tokenized attention_mask length: {len(tokenized_train_list[i]['attention_mask'])}\")\n",
    "    \n",
    "    # Decode back to text to see if it makes sense\n",
    "    decoded_text = tokenizer.decode(tokenized_train_list[i]['input_ids'], skip_special_tokens=True)\n",
    "    print(f\"Decoded text: {decoded_text[:150]}...\")\n",
    "\n",
    "# Check for problematic data\n",
    "print(\"\\n=== CHECKING FOR DATA ISSUES ===\")\n",
    "\n",
    "# 1. Check for empty sequences\n",
    "empty_sequences = [i for i, item in enumerate(tokenized_train_list) if len(item['input_ids']) == 0]\n",
    "print(f\"Empty sequences found: {len(empty_sequences)}\")\n",
    "\n",
    "# 2. Check for very short sequences (might be corrupted)\n",
    "short_sequences = [i for i, item in enumerate(tokenized_train_list) if len(item['input_ids']) < 10]\n",
    "print(f\"Very short sequences (<10 tokens): {len(short_sequences)}\")\n",
    "\n",
    "# 3. Check sequence length distribution\n",
    "lengths = [len(item['input_ids']) for item in tokenized_train_list]\n",
    "print(f\"Sequence length stats:\")\n",
    "print(f\"  Min: {min(lengths)}, Max: {max(lengths)}, Mean: {np.mean(lengths):.1f}\")\n",
    "print(f\"  Standard deviation: {np.std(lengths):.1f}\")\n",
    "\n",
    "# 4. Check if all sequences have both input_ids and attention_mask\n",
    "incomplete_sequences = [i for i, item in enumerate(tokenized_train_list) \n",
    "                       if 'input_ids' not in item or 'attention_mask' not in item]\n",
    "print(f\"Incomplete sequences: {len(incomplete_sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dc7fc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and formatting data...\n",
      "\n",
      "Sample formatted conversations:\n",
      "Example 1: User: What is the total estimated project cost mentioned in the document? Assistant: The grand total...\n",
      "Example 2: User: Where should the payment be remitted to? Assistant: The payment should be remitted to Wolf Kni...\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean text but KEEP stop words for natural conversation\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove markdown patterns\n",
    "    text = re.sub(r'#+\\s*Document Type[:]?', '', text)\n",
    "    text = re.sub(r'\\*\\*.*?\\*\\*', '', text)\n",
    "    text = re.sub(r'###\\s*', '', text)\n",
    "    text = re.sub(r'- \\*\\*', '', text)\n",
    "    \n",
    "    # Clean extra whitespace but keep natural language\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def create_conversation(example):\n",
    "    \"\"\"Format for conversational training\"\"\"\n",
    "    question = clean_text(example['question'])\n",
    "    answer = clean_text(example['answer'])\n",
    "    \n",
    "    # Simple conversational format\n",
    "    formatted_text = f\"User: {question} Assistant: {answer}{tokenizer.eos_token}\"\n",
    "    return {'text': formatted_text}\n",
    "\n",
    "print(\"Cleaning and formatting data...\")\n",
    "train_data_clean = train_data.map(create_conversation)\n",
    "val_data_clean = val_data.map(create_conversation)\n",
    "# Show some examples\n",
    "print(\"\\nSample formatted conversations:\")\n",
    "for i in range(2):\n",
    "    print(f\"Example {i+1}: {train_data_clean[i]['text'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7a57182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Tokenization (now tokenizer is defined)\n",
    "def tokenize_data(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing data...\")\n",
    "tokenized_train = train_data_clean.map(tokenize_data, batched=True, remove_columns=train_data_clean.column_names)\n",
    "tokenized_val = val_data_clean.map(tokenize_data, batched=True, remove_columns=val_data_clean.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4861f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a05c4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(tokenized_data):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    for x in tokenized_data:\n",
    "        if len(x['input_ids']) < 256:\n",
    "            pad_length = 256 - len(x['input_ids'])\n",
    "            input_ids.append(x['input_ids'] + [tokenizer.pad_token_id]*pad_length)\n",
    "            attention_mask.append(x['attention_mask'] + [0]*pad_length)\n",
    "        else:\n",
    "            input_ids.append(x['input_ids'][:256])\n",
    "            attention_mask.append(x['attention_mask'][:256])\n",
    "\n",
    "    input_ids = tf.constant(input_ids, dtype=tf.int32)\n",
    "    attention_mask = tf.constant(attention_mask, dtype=tf.int32)\n",
    "    labels = tf.constant(input_ids, dtype=tf.int32)\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices(\n",
    "        ({'input_ids': input_ids, 'attention_mask': attention_mask}, labels)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86fe92ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 17:14:08.144682: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "tf_train_dataset = create_tf_dataset(tokenized_train).shuffle(1000).batch(8)\n",
    "tf_val_dataset = create_tf_dataset(tokenized_val).batch(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b4537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "{'input_ids': [12982, 25, 1867, 318, 262, 2472, 6108, 1628, 1575, 4750, 287, 262, 3188, 30, 15286, 25, 383, 4490, 2472, 286, 262, 6108, 1628, 1575, 318, 720, 22, 11, 46438, 11, 23906, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2693600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert HF Dataset to plain list of dicts\n",
    "tokenized_train_list = tokenized_train.to_dict()['input_ids']\n",
    "tokenized_train_list = [{'input_ids': iid, 'attention_mask': am}\n",
    "                        for iid, am in zip(tokenized_train['input_ids'], tokenized_train['attention_mask'])]\n",
    "\n",
    "tokenized_val_list = [{'input_ids': iid, 'attention_mask': am}\n",
    "                      for iid, am in zip(tokenized_val['input_ids'], tokenized_val['attention_mask'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c6e326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = create_tf_dataset(tokenized_train_list).batch(8)\n",
    "tf_val_dataset = create_tf_dataset(tokenized_val_list).batch(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ccc8d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "{'input_ids': [12982, 25, 1867, 318, 262, 2472, 6108, 1628, 1575, 4750, 287, 262, 3188, 30, 15286, 25, 383, 4490, 2472, 286, 262, 6108, 1628, 1575, 318, 720, 22, 11, 46438, 11, 23906, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenized_train))       # Should ideally be list\n",
    "print(tokenized_train[0])          # Should show {'input_ids': [...], 'attention_mask': [...]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29e4dbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 model with safetensors disabled...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel\n",
    "\n",
    "print(\"Loading GPT-2 model with safetensors disabled...\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", use_safetensors=False)\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4128d6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SETTING UP MANUAL TRAINING ===\n",
      "Training samples: 5000\n",
      "Validation samples: 1000\n",
      "Preparing data arrays...\n",
      "Training data shape: (5000, 256)\n",
      "Validation data shape: (1000, 256)\n",
      "\n",
      "🚀 STARTING MANUAL TRAINING...\n",
      "This will take 30-60 minutes on CPU...\n",
      "You'll see progress updates every 100 batches\n",
      "\n",
      "🎯 Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "# MANUAL TRAINING LOOP - GUARANTEED TO WORK\n",
    "print(\"=== SETTING UP MANUAL TRAINING ===\")\n",
    "\n",
    "# Convert to lists and prepare data\n",
    "tokenized_train_list = list(tokenized_train)\n",
    "tokenized_val_list = list(tokenized_val)\n",
    "\n",
    "print(f\"Training samples: {len(tokenized_train_list)}\")\n",
    "print(f\"Validation samples: {len(tokenized_val_list)}\")\n",
    "\n",
    "# Prepare data arrays\n",
    "def prepare_data_arrays(tokenized_list):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for item in tokenized_list:\n",
    "        seq = item['input_ids']\n",
    "        mask = item['attention_mask']\n",
    "        \n",
    "        # Pad to exactly 256\n",
    "        if len(seq) < 256:\n",
    "            pad_len = 256 - len(seq)\n",
    "            input_ids.append(seq + [tokenizer.pad_token_id] * pad_len)\n",
    "            attention_mask.append(mask + [0] * pad_len)\n",
    "        else:\n",
    "            input_ids.append(seq[:256])\n",
    "            attention_mask.append(mask[:256])\n",
    "    \n",
    "    return (np.array(input_ids, dtype=np.int32), \n",
    "            np.array(attention_mask, dtype=np.int32))\n",
    "\n",
    "print(\"Preparing data arrays...\")\n",
    "train_input_ids, train_attention_mask = prepare_data_arrays(tokenized_train_list)\n",
    "val_input_ids, val_attention_mask = prepare_data_arrays(tokenized_val_list)\n",
    "\n",
    "print(f\"Training data shape: {train_input_ids.shape}\")\n",
    "print(f\"Validation data shape: {val_input_ids.shape}\")\n",
    "\n",
    "# Manual training function\n",
    "def train_manually(model, train_data, val_data, epochs=3):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    \n",
    "    train_input_ids, train_attention_mask = train_data\n",
    "    val_input_ids, val_attention_mask = val_data\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n🎯 Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        # --- TRAINING ---\n",
    "        epoch_train_loss = 0\n",
    "        num_train_batches = 0\n",
    "        \n",
    "        # Process training in batches\n",
    "        for i in range(0, len(train_input_ids), 8):\n",
    "            # Get batch\n",
    "            batch_input_ids = train_input_ids[i:i+8]\n",
    "            batch_attention_mask = train_attention_mask[i:i+8]\n",
    "            \n",
    "            # Forward pass with gradient tape\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,\n",
    "                    attention_mask=batch_attention_mask,\n",
    "                    labels=batch_input_ids  # Labels are same as input_ids for LM\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "            epoch_train_loss += loss.numpy()\n",
    "            num_train_batches += 1\n",
    "            \n",
    "            # Print progress every 100 batches\n",
    "            if num_train_batches % 100 == 0:\n",
    "                print(f\"  Training batch {num_train_batches}, Loss: {loss.numpy():.4f}\")\n",
    "        \n",
    "        avg_train_loss = epoch_train_loss / num_train_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # --- VALIDATION ---\n",
    "        epoch_val_loss = 0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        # Process validation in batches (no gradients)\n",
    "        for i in range(0, len(val_input_ids), 8):\n",
    "            batch_input_ids = val_input_ids[i:i+8]\n",
    "            batch_attention_mask = val_attention_mask[i:i+8]\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch_input_ids,\n",
    "                attention_mask=batch_attention_mask,\n",
    "                labels=batch_input_ids\n",
    "            )\n",
    "            epoch_val_loss += outputs.loss.numpy()\n",
    "            num_val_batches += 1\n",
    "        \n",
    "        avg_val_loss = epoch_val_loss / num_val_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"✅ Epoch {epoch + 1} completed:\")\n",
    "        print(f\"   Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"   Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "print(\"\\n🚀 STARTING MANUAL TRAINING...\")\n",
    "print(\"This will take 30-60 minutes on CPU...\")\n",
    "print(\"You'll see progress updates every 100 batches\")\n",
    "\n",
    "# Start manual training\n",
    "train_losses, val_losses = train_manually(\n",
    "    model,\n",
    "    (train_input_ids, train_attention_mask),\n",
    "    (val_input_ids, val_attention_mask),\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 TRAINING COMPLETED!\")\n",
    "print(\"Final losses:\")\n",
    "print(f\"  Training: {train_losses[-1]:.4f}\")\n",
    "print(f\"  Validation: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df92051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01988dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c581b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
